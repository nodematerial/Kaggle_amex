{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LightGBM Quickstart for the *American Express - Default Prediction* competition\n\nThis notebook shows how to apply LightGBM to the competition data, and it introduces a space-efficient way of feature engineering.\n\nIt is based on the [EDA which makes sense ⭐️⭐️⭐️⭐️⭐️](https://www.kaggle.com/code/ambrosm/amex-eda-which-makes-sense).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib.colors import ListedColormap\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\nimport warnings\nfrom colorama import Fore, Back, Style\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.calibration import CalibrationDisplay\nfrom lightgbm import LGBMClassifier, log_evaluation\n\nplt.rcParams['axes.facecolor'] = '#0057b8' # blue\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])\nplt.rcParams['text.color'] = 'w'\n\nINFERENCE = True # set to False if you only want to cross-validate\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-07T17:44:54.370455Z","iopub.execute_input":"2022-06-07T17:44:54.371021Z","iopub.status.idle":"2022-06-07T17:44:56.043444Z","shell.execute_reply.started":"2022-06-07T17:44:54.370913Z","shell.execute_reply":"2022-06-07T17:44:56.042316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @yunchonggan's fast metric implementation\n# From https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\ndef amex_metric(y_true: np.array, y_pred: np.array) -> float:\n\n    # count of positives and negatives\n    n_pos = y_true.sum()\n    n_neg = y_true.shape[0] - n_pos\n\n    # sorting by descring prediction values\n    indices = np.argsort(y_pred)[::-1]\n    preds, target = y_pred[indices], y_true[indices]\n\n    # filter the top 4% by cumulative row weights\n    weight = 20.0 - target * 19.0\n    cum_norm_weight = (weight / weight.sum()).cumsum()\n    four_pct_filter = cum_norm_weight <= 0.04\n\n    # default rate captured at 4%\n    d = target[four_pct_filter].sum() / n_pos\n\n    # weighted gini coefficient\n    lorentz = (target / n_pos).cumsum()\n    gini = ((lorentz - cum_norm_weight) * weight).sum()\n\n    # max weighted gini coefficient\n    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n\n    # normalized weighted gini coefficient\n    g = gini / gini_max\n\n    return 0.5 * (g + d)\n\ndef lgb_amex_metric(y_true, y_pred):\n    \"\"\"The competition metric with lightgbm's calling convention\"\"\"\n    return ('amex',\n            amex_metric(y_true, y_pred),\n            True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-07T17:44:56.044902Z","iopub.execute_input":"2022-06-07T17:44:56.045233Z","iopub.status.idle":"2022-06-07T17:44:56.054244Z","shell.execute_reply.started":"2022-06-07T17:44:56.045206Z","shell.execute_reply":"2022-06-07T17:44:56.05325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading and preprocessing the data\n\nWe read the data from @raddar's [dataset](https://www.kaggle.com/datasets/raddar/amex-data-integer-dtypes-parquet-format). @raddar has [denoised the data](https://www.kaggle.com/competitions/amex-default-prediction/discussion/328514) so that we can achieve better results with his dataset than with the original competition csv files.\n\nThen we create three groups of features:\n- Selected features averaged over all statements of a customer\n- The maximum of selected features over all statements of a customer\n- Selected features taken from the last statement of a customer\n\nThe code has been optimized for memory efficiency rather than readability. In particular, `.iloc[mask_array, columns]` needs much less RAM than the groupby construction used in a previous version of the notebook.\n\nPreprocessing for LightGBM is much simpler than for neural networks:\n1. Neural networks can't process missing values; LightGBM handles them automatically.\n1. Categorical features need to be one-hot encoded for neural networks; LightGBM handles them automatically.\n1. With neural networks, you need to think about outliers; tree-based algorithms deal with outliers easily.\n1. Neural networks need scaled inputs; tree-based algorithms don't depend on scaling.","metadata":{}},{"cell_type":"code","source":"#%%time\nfeatures_avg = ['B_1', 'B_11', 'B_16', 'B_17', 'B_18', 'B_2', 'B_20',\n                'B_28', 'B_3', 'B_4', 'B_5', 'B_7', 'B_9', 'D_112',\n                'D_121', 'D_141', 'D_39', 'D_41', 'D_42', 'D_43',\n                'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', \n                'D_50', 'D_51', 'D_53', 'D_54', 'D_56', 'D_58', \n                'D_59', 'D_60', 'D_91', 'P_2', 'P_3', 'R_1', 'R_2', \n                'R_27', 'R_3', 'R_7', 'S_11', 'S_26', 'S_3', 'S_5']\nfeatures_max = ['B_1', 'B_11', 'B_13', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', \n                'B_22', 'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_31', 'B_33', 'B_36', \n                'B_4', 'B_42', 'B_5', 'B_7', 'B_9', 'D_102', 'D_103', 'D_105', 'D_109', \n                'D_110', 'D_112', 'D_113', 'D_115', 'D_121', 'D_124', 'D_128', 'D_129', \n                'D_131', 'D_139', 'D_141', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', \n                'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_50', 'D_51', 'D_52', \n                'D_53', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', 'D_72', 'D_74', \n                'D_75', 'D_79', 'D_81', 'D_83', 'D_84', 'D_88', 'D_89', 'P_2', 'P_3', \n                'R_1', 'R_10', 'R_11', 'R_26', 'R_28', 'R_3', 'R_4', 'R_5', 'R_7', 'R_8', \n                'S_11', 'S_12', 'S_23', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_7', 'S_8', ]\nfeatures_last = ['B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_15', 'B_16',\n                 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_22', 'B_23',\n                 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3',\n                 'B_32', 'B_33', 'B_36', 'B_38', 'B_39', 'B_4', 'B_40',\n                 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9',\n                 'D_102', 'D_103', 'D_105', 'D_106', 'D_107', 'D_109',\n                 'D_112', 'D_115', 'D_117', 'D_118', 'D_119', 'D_120',\n                 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127', \n                 'D_129', 'D_132', 'D_133', 'D_135', 'D_136', 'D_137', \n                 'D_140', 'D_141', 'D_143', 'D_145', 'D_39', 'D_41',\n                 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48',\n                 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55',\n                 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63',\n                 'D_64', 'D_66', 'D_70', 'D_72', 'D_73', 'D_74', 'D_75',\n                 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_82', 'D_83',\n                 'D_84', 'D_86', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96',\n                 'P_2', 'P_3', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13',\n                 'R_14', 'R_15', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', \n                 'R_21', 'R_22', 'R_24', 'R_25', 'R_26', 'R_27', 'R_3',\n                 'R_4', 'R_5', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12',\n                 'S_13', 'S_15', 'S_17', 'S_20', 'S_22', 'S_23', \n                 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6',\n                 'S_7', 'S_8', 'S_9']\n\nfor i in ['test', 'train'] if INFERENCE else ['train']:\n    df = pd.read_parquet(f'../input/amex-data-integer-dtypes-parquet-format/{i}.parquet')\n    cid = pd.Categorical(df.pop('customer_ID'), ordered=True)\n    last = (cid != np.roll(cid, -1)) # mask for last statement of every customer\n    if 'target' in df.columns:\n        df.drop(columns=['target'], inplace=True)\n    gc.collect()\n    print('Read', i)\n    df_avg = (df\n              .groupby(cid)\n              .mean()[features_avg]\n              .rename(columns={f: f\"{f}_avg\" for f in features_avg})\n             )\n    gc.collect()\n    print('Computed avg', i)\n    df_max = (df\n              .groupby(cid)\n              .max()[features_max]\n              .rename(columns={f: f\"{f}_max\" for f in features_max})\n             )\n    gc.collect()\n    print('Computed max', i)\n    df = (df.loc[last, features_last]\n          .rename(columns={f: f\"{f}_last\" for f in features_last})\n          .set_index(np.asarray(cid[last]))\n         )\n    gc.collect()\n    print('Computed last', i)\n    df = pd.concat([df, df_max, df_avg], axis=1)\n    if i == 'train': train = df\n    else: test = df\n    print(f\"{i} shape: {df.shape}\")\n    del df, df_avg, df_max, cid, last\n\ntarget = pd.read_csv('../input/amex-default-prediction/train_labels.csv').target.values\nprint(f\"target shape: {target.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T17:44:56.055743Z","iopub.execute_input":"2022-06-07T17:44:56.056329Z","iopub.status.idle":"2022-06-07T17:49:31.422609Z","shell.execute_reply.started":"2022-06-07T17:44:56.056289Z","shell.execute_reply":"2022-06-07T17:49:31.421028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation\n\nWe cross-validate with a five-fold StratifiedKFold because the classes are imbalanced.\n\nNotice that lightgbm logs the validation score with the competition's scoring function every hundred iterations.","metadata":{}},{"cell_type":"code","source":"%%time\n# Cross-validation of the classifier\n\nONLY_FIRST_FOLD = False\n\nfeatures = [f for f in train.columns if f != 'customer_ID' and f != 'target']\n\ndef my_booster(random_state=1, n_estimators=1200):\n    return LGBMClassifier(n_estimators=n_estimators,\n                          learning_rate=0.03, reg_lambda=50,\n                          min_child_samples=2400,\n                          num_leaves=95,\n                          max_bins=511, random_state=random_state)\n      \nprint(f\"{len(features)} features\")\nscore_list = []\ny_pred_list = []\nkf = StratifiedKFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train, target)):\n    X_tr, X_va, y_tr, y_va, model = None, None, None, None, None\n    start_time = datetime.datetime.now()\n    X_tr = train.iloc[idx_tr][features]\n    X_va = train.iloc[idx_va][features]\n    y_tr = target[idx_tr]\n    y_va = target[idx_va]\n    \n    model = my_booster()\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=UserWarning)\n        model.fit(X_tr, y_tr,\n                  eval_set = [(X_va, y_va)], \n                  eval_metric=[lgb_amex_metric],\n                  callbacks=[log_evaluation(10)])\n    X_tr, y_tr = None, None\n    y_va_pred = model.predict_proba(X_va, raw_score=True)\n    score = amex_metric(y_va, y_va_pred)\n    n_trees = model.best_iteration_\n    if n_trees is None: n_trees = model.n_estimators\n    print(f\"{Fore.GREEN}{Style.BRIGHT}Fold {fold} | {str(datetime.datetime.now() - start_time)[-12:-7]} |\"\n          f\" {n_trees:5} trees |\"\n          f\"                Score = {score:.5f}{Style.RESET_ALL}\")\n    score_list.append(score)\n    \n    if INFERENCE:\n        y_pred_list.append(model.predict_proba(test[features], raw_score=True))\n        \n    if ONLY_FIRST_FOLD: break # we only want the first fold\n    \nprint(f\"{Fore.GREEN}{Style.BRIGHT}OOF Score:                       {np.mean(score_list):.5f}{Style.RESET_ALL}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T17:49:31.424787Z","iopub.execute_input":"2022-06-07T17:49:31.425563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction histogram","metadata":{}},{"cell_type":"code","source":"def sigmoid(log_odds):\n    return 1 / (1 + np.exp(-log_odds))\n\nplt.figure(figsize=(10, 4))\nplt.hist(sigmoid(y_va_pred[y_va == 0]), bins=np.linspace(0, 1, 21),\n         alpha=0.5, density=True, label='0')\nplt.hist(sigmoid(y_va_pred[y_va == 1]), bins=np.linspace(0, 1, 21),\n         alpha=0.5, density=True, label='1')\nplt.xlabel('y_pred')\nplt.ylabel('density')\nplt.title('OOF Prediction histogram', color='k')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calibration diagram\n\nThe calibration diagram shows how the model predicts the default probability of customers:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nCalibrationDisplay.from_predictions(y_va, sigmoid(y_va_pred), n_bins=50,\n                                    strategy='quantile', ax=plt.gca())\nplt.title('Probability calibration')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nWe submit the mean of the five predictions. As proposed by @lucasmorin, we [take the mean of the log odds](https://www.kaggle.com/competitions/amex-default-prediction/discussion/329103) rather than of the probabilities.","metadata":{}},{"cell_type":"code","source":"if INFERENCE:\n    sub = pd.DataFrame({'customer_ID': test.index,\n                        'prediction': np.mean(y_pred_list, axis=0)})\n    sub.to_csv('submission.csv', index=False)\n    display(sub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}